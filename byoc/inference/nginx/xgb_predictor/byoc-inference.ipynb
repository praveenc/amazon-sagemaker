{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a485b55-4b06-4fa9-8148-0bb37603eb6a",
   "metadata": {},
   "source": [
    "# SageMaker BYO Inference container (nginx, gunicorn, Flask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def65a6-796a-482a-a934-96b77dd93f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U awscli boto3 sagemaker rich rich-cli watermark --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543a963-bfad-41ff-8073-436c0b7be4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%load_ext rich\n",
    "\n",
    "%watermark -p awscli,boto3,sagemaker,xgboost,sklearn,rich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838fec2-f74a-4186-95b6-6b486afe1a7e",
   "metadata": {},
   "source": [
    "### Build and test custom inference image locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38730055-fd66-40ce-a520-dd44bcb16ab7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba8ab1-4e0e-41f5-ad4e-e647b2a13efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build a local image\n",
    "!docker build -t nginx ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd93d6-e00a-44ce-a742-79e1bc5b4076",
   "metadata": {},
   "source": [
    "### Launch Inference container locally\n",
    "\n",
    "- Mount the [models](./models) directory to `/opt/ml/model` directory inside the container\n",
    "- Map container port 8080 to host port 8080\n",
    "- `docker run --rm -v $(pwd)/models:/opt/ml/model -p 8080:8080 nginx`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef0220-ae37-4d72-b230-a292498c73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open terminal and run this command to launch container locally\n",
    "# docker run --rm -v $(pwd)/models:/opt/ml/model -p 8080:8080 nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd99bed-c9a6-4194-b41f-54501aba6816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ping local inference endpoint\n",
    "!curl http://localhost:8080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902de0e4-03da-4592-82c0-461d1ee17b51",
   "metadata": {},
   "source": [
    "#### Test records for inference can be found in [test.csv](./test.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365d9de-51fe-46dc-9e5e-a82b05685bcc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Send test records to /invocations on the endpoint\n",
    "!curl --data-raw '-1.3317586042173168,-1.1425409076053987,-1.0579488602777858,-1.177706547272754,-1.130662184748842,-1.1493955859050584,-1.139968767909096,0.0,1.0,0.0' -H 'Content-Type: text/csv' \\\n",
    "-v http://localhost:8080/invocations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad32979-3897-42e7-94f4-8537e12c720a",
   "metadata": {},
   "source": [
    "### Tag and push the local image to private ECR\n",
    "\n",
    "Now tag the `nginx` local image to ECR format `{account_id}.dkr.ecr.{region}.amazonaws.com/{imagename}:{tag}` format\n",
    "\n",
    "Run [./build_n_push.sh](./build_n_push.sh) shell script with image name `nginx` as parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863efc3-4dfd-4ecc-8206-39f7b51ea690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod +x ./build_n_push.sh\n",
    "!./build_n_push.sh nginx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a336a71-b05e-484c-8609-2d26cd1f7a1a",
   "metadata": {},
   "source": [
    "## Deploy your model to SageMaker Endpoint using custom inference image\n",
    "\n",
    "- Step 1: SageMaker session initialize\n",
    "- Step 2: Compress your model to `model.tar.gz` format and upload to s3\n",
    "- Step 3: Create Model object with your custom inference image \n",
    "- Step 4: Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1957a055-ae4d-4003-8bd3-4a8dbb4267d2",
   "metadata": {},
   "source": [
    "#### Step 1: Initialize Session and upload model artifacts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bff45f-56e3-4bc1-90e8-0d9e31def130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "import tarfile\n",
    "from rich import print\n",
    "from sagemaker import get_execution_role, session\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader, s3_path_join\n",
    "\n",
    "sm_session = session.Session()\n",
    "region = sm_session._region_name\n",
    "role = get_execution_role()\n",
    "bucket = sm_session.default_bucket()\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "prefix = \"sagemaker/abalone\"\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "model_s3uri = s3_path_join(f\"s3://{bucket}/{prefix}\", \"models/byoc/nginx\")\n",
    "\n",
    "print(f\"Role: {role}\")\n",
    "print(f\"Bucket: {bucket}\")\n",
    "print(f\"Model base: {model_s3uri}\")\n",
    "\n",
    "S3Downloader.list(model_s3uri)\n",
    "\n",
    "model_path = os.path.join(\"./models\", \"xgboost-model\")\n",
    "model_output_path = os.path.join(\"./models\", \"model.tar.gz\")\n",
    "\n",
    "if not os.path.exists(model_output_path):\n",
    "    print(f\"Compressing model to {model_output_path}\")\n",
    "    tar = tarfile.open(model_output_path, \"w:gz\")\n",
    "    tar.add(model_path, arcname=\"xgboost-model\")\n",
    "    tar.close()\n",
    "else:\n",
    "    print(f\"Model file exists: {model_output_path}\")\n",
    "\n",
    "S3Uploader.upload(\n",
    "    local_path=model_output_path,\n",
    "    desired_s3_uri=model_s3uri,\n",
    "    sagemaker_session=sm_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04630460-d038-4690-bd6d-37a2b3838b11",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 2: Create model object with custom inference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c37340-474b-4c37-b7af-d3063235c57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "from sagemaker.model import Model\n",
    "\n",
    "image_name = \"nginx\"\n",
    "ecr_image = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{image_name}:latest\"\n",
    "suffix = f\"{str(uuid4())[:5]}-{datetime.now().strftime('%d%b%Y')}\"\n",
    "\n",
    "model_data_url = s3_path_join(model_s3uri, \"model.tar.gz\")\n",
    "print(f\"model_image_uri: {ecr_image}\")\n",
    "model_name = f\"AbaloneXGB-predictor-{suffix}\"\n",
    "\n",
    "print(f\"Creating model : {model_name} with {model_data_url}\")\n",
    "\n",
    "predictor_model = Model(\n",
    "    image_uri=ecr_image,\n",
    "    name=model_name,\n",
    "    model_data=model_data_url,\n",
    "    role=role,\n",
    "    sagemaker_session=sm_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29ddc7-7562-4a55-b96d-5fab1f3b2bfa",
   "metadata": {},
   "source": [
    "#### Step 3: Deploy model to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51248e7f-8c38-4b6b-a3db-7d6b61c5a04a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = f\"Abalone-nginx-ep-{suffix}\"\n",
    "\n",
    "print(f\"Deploying model: {model_name}\")\n",
    "predictor = predictor_model.deploy(\n",
    "    endpoint_name=endpoint_name, initial_instance_count=1, instance_type=\"ml.m5.xlarge\", wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58952b53-ca5d-45c4-b3e7-3c73f47eb071",
   "metadata": {},
   "source": [
    "### Wait for endpoint to be InService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358fb5a-8c43-48e1-88f7-e1f4db2047bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rich import print\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(f\"Endpoint [b]{endpoint_name}[/b] Status: [i]{status}[/i]\")\n",
    "\n",
    "# Get the waiter object\n",
    "waiter = sm_client.get_waiter(\"endpoint_in_service\")\n",
    "# Apply the waiter on the endpoint\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "\n",
    "# Get endpoint status using describe endpoint\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(f\"Endpoint [b]{endpoint_name}[/b] Status: [i]{status}[/i]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9786cc6d-3298-42c6-ac83-423474011eed",
   "metadata": {},
   "source": [
    "### Test real-time endpoint on SageMaker with inference records\n",
    "[test.csv](./test.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717ff77-2700-4af7-b143-0de0e67bff2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from time import sleep, time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "LOCALDIR = \".\"\n",
    "\n",
    "local_test_dataset = f\"{LOCALDIR}/test.csv\"\n",
    "\n",
    "limit = 10\n",
    "i = 0\n",
    "\n",
    "with open(local_test_dataset, \"r\") as _f:\n",
    "    for row in _f:\n",
    "        if i == 0:\n",
    "            print(f\"Headers\")\n",
    "            print(row)\n",
    "            print(\"---\" * 20)\n",
    "            i += 1\n",
    "        elif i <= limit:\n",
    "            row = row.rstrip(\"\\n\")\n",
    "            splits = row.split(\",\")\n",
    "            # Remove the target column (first column)\n",
    "            label = splits.pop(0)\n",
    "            input_cols = \",\".join(s for s in splits)\n",
    "            prediction = None\n",
    "            try:\n",
    "                print(f\"Invoking EP with record\")\n",
    "                # print(input_cols)\n",
    "                prediction = runtime_sm_client.invoke_endpoint(\n",
    "                    EndpointName=endpoint_name,\n",
    "                    ContentType=\"text/csv\",\n",
    "                    Body=input_cols,\n",
    "                )\n",
    "                # print(prediction[\"Body\"].read())\n",
    "                response = prediction[\"Body\"].read().decode(\"utf-8\")\n",
    "                # print(response)\n",
    "                print(f\"True: {label} | Predicted: {response}\")\n",
    "                i += 1\n",
    "                sleep(0.5)\n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error: {e}\")\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed1695-39b8-497d-a260-027ac6de23d6",
   "metadata": {},
   "source": [
    "### Verify Logs emitted by the endpoint in CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1dd365-391a-4797-b4b4-5d74e75dfd85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "logs_client = boto3.client(\"logs\")\n",
    "end_time = datetime.utcnow()\n",
    "start_time = end_time - timedelta(minutes=15)\n",
    "\n",
    "log_group_name = f\"/aws/sagemaker/Endpoints/{endpoint_name}\"\n",
    "log_streams = logs_client.describe_log_streams(logGroupName=log_group_name)\n",
    "log_stream_name = log_streams[\"logStreams\"][0][\"logStreamName\"]\n",
    "\n",
    "# Retrieve the logs\n",
    "logs = logs_client.get_log_events(\n",
    "    logGroupName=log_group_name,\n",
    "    logStreamName=log_stream_name,\n",
    "    startTime=int(start_time.timestamp() * 1000),\n",
    "    endTime=int(end_time.timestamp() * 1000),\n",
    ")\n",
    "\n",
    "# Print the logs\n",
    "for event in logs[\"events\"]:\n",
    "    print(f\"{datetime.fromtimestamp(event['timestamp'] // 1000)}: {event['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05d4caf-e665-477d-965d-1ba8cb91bd0a",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ffc175-ee55-4dc7-83ec-b306a75978e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete endpoint, endpoint_configuration and model\n",
    "print(f\"EP: {endpoint_name}\")\n",
    "\n",
    "try:\n",
    "    print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting EP: {endpoint_name}\\n{e}\")\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
